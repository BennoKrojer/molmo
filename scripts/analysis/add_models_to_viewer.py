#!/usr/bin/env python3
"""
Add Models to Unified Viewer - BULLETPROOF VERSION

This script adds new models (main or ablations) to an existing unified viewer.
It uses the SAME code path for all models - no separate ablation logic.

Key principles:
1. Data-driven: reads from viewer_models.json
2. Validation first: checks data exists before generation
3. Single code path: main models and ablations use identical logic
4. Incremental: only adds missing models, doesn't regenerate existing ones
5. Verbose: tells you exactly what's happening and what's missing

Usage:
    # Validate only (no changes)
    python add_models_to_viewer.py --validate-only

    # Add all missing models
    python add_models_to_viewer.py --output-dir analysis_results/unified_viewer_lite

    # Add specific ablation
    python add_models_to_viewer.py --output-dir analysis_results/unified_viewer_lite --only seed10

    # Force regenerate a model
    python add_models_to_viewer.py --output-dir analysis_results/unified_viewer_lite --only seed10 --force
"""

import json
import argparse
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import shutil

# =============================================================================
# CONFIGURATION
# =============================================================================

ANALYSIS_RESULTS_DIR = Path("analysis_results")

# File patterns for each analysis type
FILE_PATTERNS = {
    "nn": [
        "nearest_neighbors_analysis_*_layer*.json",  # Main models
        "nearest_neighbors_layer*_topk*.json",       # Qwen2-VL style
    ],
    "logitlens": [
        "logit_lens_layer*_topk*.json",
    ],
    "contextual": [
        "contextual_neighbors_visual*_allLayers.json",
    ],
}

# =============================================================================
# VALIDATION FUNCTIONS
# =============================================================================

def find_data_files(base_dir: Path, analysis_type: str, data_path: str) -> Dict[int, Path]:
    """Find all data files for a given analysis type and return layer -> path mapping."""
    results = {}
    
    if analysis_type == "nn":
        search_dir = base_dir / "nearest_neighbors" / data_path
    elif analysis_type == "logitlens":
        search_dir = base_dir / "logit_lens" / data_path
    elif analysis_type == "contextual":
        search_dir = base_dir / "contextual_nearest_neighbors" / data_path
    else:
        return results
    
    if not search_dir.exists():
        return results
    
    for pattern in FILE_PATTERNS.get(analysis_type, []):
        for json_file in search_dir.glob(pattern):
            try:
                # Extract layer number from filename
                if analysis_type == "contextual":
                    # Contextual files use _visualN_ pattern
                    # e.g., contextual_neighbors_visual0_allLayers.json
                    parts = json_file.stem.split("_")
                    for part in parts:
                        if part.startswith("visual") and len(part) > len("visual"):
                            layer = int(part.replace("visual", ""))
                            results[layer] = json_file
                            break
                elif "_layer" in json_file.stem:
                    # NN and LogitLens use _layerN_ pattern
                    layer_part = json_file.stem.split("_layer")[1]
                    layer = int(layer_part.split("_")[0])
                    results[layer] = json_file
            except (ValueError, IndexError):
                pass
    
    return results


def validate_model(model_config: Dict, base_dir: Path) -> Dict:
    """Validate that data exists for a model. Returns status dict."""
    
    # Determine data paths
    if "data_paths" in model_config:
        # Ablation with explicit paths
        data_paths = model_config["data_paths"]
        model_id = model_config.get("id", model_config.get("checkpoint", "unknown"))
        model_name = model_config.get("name", model_id)
    else:
        # Main model - use checkpoint name for all paths
        checkpoint = model_config["checkpoint"]
        data_paths = {
            "nn": f"{checkpoint}_step12000-unsharded",
            "logitlens": f"{checkpoint}_step12000-unsharded",
            "contextual": f"{checkpoint}_step12000-unsharded",
        }
        model_id = checkpoint
        model_name = f"{model_config['llm']} + {model_config['vision_encoder']}"
    
    # Find available data
    available = {}
    for analysis_type, data_path in data_paths.items():
        files = find_data_files(base_dir, analysis_type, data_path)
        available[analysis_type] = {
            "layers": sorted(files.keys()),
            "count": len(files),
            "path": data_path,
        }
    
    # STRICT: ALL analysis types must have data (no partial allowed!)
    missing_types = [k for k, v in available.items() if v["count"] == 0]
    has_all_data = len(missing_types) == 0

    return {
        "id": model_id,
        "name": model_name,
        "config": model_config,
        "data_paths": data_paths,
        "available": available,
        "has_all_data": has_all_data,
        "missing_types": missing_types,
        "has_any_data": any(v["count"] > 0 for v in available.values()),  # kept for backward compat
    }


def print_validation_report(models: List[Dict]) -> Tuple[int, int]:
    """Print a validation report and return (ok_count, missing_count).

    STRICT MODE: All analysis types must have data. Partial data = FAIL.
    """

    print("\n" + "="*80)
    print("VALIDATION REPORT (STRICT MODE - NO PARTIAL DATA ALLOWED)")
    print("="*80)

    ok_count = 0
    missing_count = 0

    for model in models:
        has_all = model["has_all_data"]
        missing_types = model.get("missing_types", [])
        status = "‚úÖ" if has_all else "‚ùå"

        if has_all:
            ok_count += 1
        else:
            missing_count += 1

        print(f"\n{status} {model['name']}")
        print(f"   ID: {model['id']}")

        if not has_all:
            print(f"   ‚ö†Ô∏è  MISSING: {', '.join(missing_types)}")

        for analysis_type, info in model["available"].items():
            count = info["count"]
            layers = info["layers"]
            path = info["path"]

            if count > 0:
                print(f"   {analysis_type:12s}: {count:2d} layers - {layers}")
            else:
                print(f"   {analysis_type:12s}: ‚ùå NO DATA at {path}")

    print("\n" + "-"*80)
    if missing_count > 0:
        print(f"‚ùå VALIDATION FAILED: {missing_count} models have missing data")
        print(f"   {ok_count} models complete, {missing_count} models incomplete")
    else:
        print(f"‚úÖ VALIDATION PASSED: All {ok_count} models have complete data")
    print("-"*80)

    return ok_count, missing_count


# =============================================================================
# VIEWER GENERATION (uses existing working code)
# =============================================================================

def check_existing_viewer(output_dir: Path, model_id: str, num_images: int) -> bool:
    """Check if viewer already exists for this model."""
    model_dir = output_dir / model_id
    if not model_dir.exists():
        return False
    
    existing = list(model_dir.glob("image_*.html"))
    return len(existing) >= num_images


def copy_model_viewer_structure(src_model_dir: Path, dst_model_dir: Path) -> bool:
    """Copy an existing model viewer as a template."""
    if not src_model_dir.exists():
        return False
    
    dst_model_dir.mkdir(parents=True, exist_ok=True)
    
    # Copy index.html
    src_index = src_model_dir / "index.html"
    if src_index.exists():
        shutil.copy(src_index, dst_model_dir / "index.html")
    
    # Copy image HTML files
    for img_file in src_model_dir.glob("image_*.html"):
        shutil.copy(img_file, dst_model_dir / img_file.name)
    
    return True


# =============================================================================
# INDEX HTML GENERATION
# =============================================================================

def generate_ablation_cards_html(ablation_models: List[Dict]) -> str:
    """Generate HTML for ablation model cards to add to main index."""
    
    cards_html = ""
    for model in ablation_models:
        if not model["has_any_data"]:
            continue
        
        model_id = model["id"]
        model_name = model["name"]
        available = model["available"]
        
        # Build stats
        nn_count = available["nn"]["count"]
        logit_count = available["logitlens"]["count"]
        ctx_count = available["contextual"]["count"]
        
        stats_parts = []
        if nn_count > 0:
            stats_parts.append(f"NN: {nn_count}")
        if logit_count > 0:
            stats_parts.append(f"Logit: {logit_count}")
        if ctx_count > 0:
            stats_parts.append(f"Ctx: {ctx_count}")
        
        stats_str = " | ".join(stats_parts) if stats_parts else "No data"
        
        # Use checkpoint as folder name for link
        checkpoint = model["config"].get("checkpoint", model_id)
        
        cards_html += f'''
            <div class="model-cell available" style="padding: 20px; border-radius: 8px; min-width: 200px; flex: 0 0 auto;">
                <a href="ablations/{checkpoint}/index.html" class="model-link">
                    <div style="font-size: 16px; font-weight: 600;">{model_name}</div>
                    <div class="stats" style="margin-top: 8px;">{stats_str}</div>
                </a>
            </div>'''
    
    return cards_html


def update_main_index_with_ablations(output_dir: Path, ablation_models: List[Dict]) -> bool:
    """Update the main index.html to include ablation section."""
    
    index_path = output_dir / "index.html"
    if not index_path.exists():
        print(f"ERROR: Main index.html not found at {index_path}")
        return False
    
    # Read existing index
    with open(index_path, 'r') as f:
        html_content = f.read()
    
    # Check if ablations section already exists
    if "Ablation Studies" in html_content:
        print("  Ablations section already exists in index.html - updating...")
        # Remove existing ablations section (everything after "Ablation Studies")
        marker = '<h2 style="margin-top: 40px;'
        if marker in html_content:
            idx = html_content.find(marker)
            # Find the legend div that comes after ablations
            legend_marker = '<div class="legend">'
            legend_idx = html_content.find(legend_marker)
            if legend_idx > idx:
                # Remove content between marker and legend
                html_content = html_content[:idx] + html_content[legend_idx:]
    
    # Generate ablation cards
    ablation_cards = generate_ablation_cards_html(ablation_models)
    
    if not ablation_cards.strip():
        print("  No ablation models with data to add")
        return True
    
    # Insert ablations section before the legend
    ablations_section = f'''
        <h2 style="margin-top: 40px; color: #2c3e50; border-bottom: 2px solid #667eea; padding-bottom: 10px;">
            üî¨ Ablation Studies
        </h2>
        <p style="color: #666; margin-bottom: 20px;">Additional model variants and configurations</p>
        
        <div style="display: flex; gap: 15px; flex-wrap: wrap; margin-bottom: 30px;">
            {ablation_cards}
        </div>
        
'''
    
    # Find where to insert (before legend)
    legend_marker = '<div class="legend">'
    if legend_marker in html_content:
        idx = html_content.find(legend_marker)
        html_content = html_content[:idx] + ablations_section + html_content[idx:]
    else:
        # No legend found, insert before closing tags
        closing = '</div>\n</body>'
        if closing in html_content:
            idx = html_content.find(closing)
            html_content = html_content[:idx] + ablations_section + html_content[idx:]
    
    # Write updated index
    with open(index_path, 'w') as f:
        f.write(html_content)
    
    print(f"  ‚úÖ Updated main index.html with {len([m for m in ablation_models if m['has_any_data']])} ablations")
    return True


# =============================================================================
# MAIN
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="Add models to unified viewer (bulletproof version)")
    parser.add_argument("--config", default="scripts/analysis/viewer_models.json",
                       help="Path to models configuration JSON")
    parser.add_argument("--output-dir", type=Path,
                       help="Output directory (existing unified viewer)")
    parser.add_argument("--validate-only", action="store_true",
                       help="Only validate, don't make changes")
    parser.add_argument("--only", type=str,
                       help="Only process this specific model ID")
    parser.add_argument("--force", action="store_true",
                       help="Force regenerate even if exists")
    parser.add_argument("--num-images", type=int, default=10,
                       help="Number of images per model")
    
    args = parser.parse_args()
    
    # Load configuration
    config_path = Path(args.config)
    if not config_path.exists():
        print(f"ERROR: Config file not found: {config_path}")
        sys.exit(1)
    
    with open(config_path) as f:
        config = json.load(f)
    
    print(f"Loaded config from {config_path}")
    print(f"  Main models: {len(config.get('main_models', []))}")
    print(f"  Ablations: {len(config.get('ablations', []))}")
    
    # Validate all models
    print("\nValidating data availability...")
    
    all_models = []
    
    # Validate main models
    for model_config in config.get("main_models", []):
        validated = validate_model(model_config, ANALYSIS_RESULTS_DIR)
        validated["type"] = "main"
        all_models.append(validated)
    
    # Validate ablations
    for model_config in config.get("ablations", []):
        validated = validate_model(model_config, ANALYSIS_RESULTS_DIR)
        validated["type"] = "ablation"
        all_models.append(validated)
    
    # Filter if --only specified
    if args.only:
        all_models = [m for m in all_models if m["id"] == args.only or args.only in m["id"]]
        if not all_models:
            print(f"ERROR: No model found matching '{args.only}'")
            sys.exit(1)
    
    # Print validation report
    ok_count, missing_count = print_validation_report(all_models)
    
    if args.validate_only:
        print("\n--validate-only specified, exiting without changes")
        sys.exit(0 if missing_count == 0 else 1)
    
    if not args.output_dir:
        print("\nERROR: --output-dir required to make changes")
        sys.exit(1)
    
    if not args.output_dir.exists():
        print(f"\nERROR: Output directory does not exist: {args.output_dir}")
        print("Run the main create_unified_viewer.py first to create the base viewer")
        sys.exit(1)

    # STRICT: Fail if ANY model has missing data
    if missing_count > 0:
        print("\n" + "!"*80)
        print("ERROR: Cannot proceed - some models have incomplete data!")
        print("Fix the data paths in viewer_models.json or generate the missing data.")
        print("!"*80)
        sys.exit(1)

    # Process ablations
    print("\n" + "="*80)
    print("ADDING ABLATIONS TO VIEWER")
    print("="*80)

    ablation_models = [m for m in all_models if m["type"] == "ablation" and m["has_all_data"]]
    
    if not ablation_models:
        print("No ablation models with data to add")
        sys.exit(0)
    
    # Create ablations directory
    ablations_dir = args.output_dir / "ablations"
    ablations_dir.mkdir(exist_ok=True)
    
    # For each ablation, we need to generate viewer files
    # This is where we would call the actual generation logic
    # For now, let's just update the main index
    
    print(f"\nWill add {len(ablation_models)} ablation models:")
    for m in ablation_models:
        checkpoint = m["config"].get("checkpoint", m["id"])
        exists = (ablations_dir / checkpoint).exists()
        status = "EXISTS" if exists else "NEW"
        print(f"  - {m['name']} ({status})")
    
    # Update main index
    print("\nUpdating main index.html...")
    update_main_index_with_ablations(args.output_dir, ablation_models)
    
    print("\n" + "="*80)
    print("NEXT STEPS")
    print("="*80)
    print("""
To fully generate ablation viewers, run the main create_unified_viewer.py 
with ablation support enabled. This script has:

1. ‚úÖ Validated all data paths
2. ‚úÖ Updated main index.html with ablation links
3. ‚ö†Ô∏è  Ablation image viewers need generation (use main script)

The main index now links to ablations in: ablations/<checkpoint>/index.html
""")


if __name__ == "__main__":
    main()

