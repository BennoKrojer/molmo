{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V-Lens Paper Figures\n",
        "\n",
        "This notebook contains all the data and plotting logic for the figures in the paper:\n",
        "**\"The surprising interpretability of vision tokens in LLMs\"**\n",
        "\n",
        "Each section contains:\n",
        "1. The raw data needed for the plot\n",
        "2. The plotting code\n",
        "\n",
        "This notebook is self-contained and reproducible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Set default style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "\n",
        "# Output directory for figures\n",
        "OUTPUT_DIR = Path('paper_figures_output')\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "print(f\"Figures will be saved to: {OUTPUT_DIR.absolute()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Figure 1: Unified Interpretability Line Plot\n",
        "\n",
        "Three-panel figure showing interpretability across layers for:\n",
        "- (a) Nearest Neighbors (static V-Lens at input layer)\n",
        "- (b) Logit Lens\n",
        "- (c) Contextual NN (contextual V-Lens)\n",
        "\n",
        "Each panel shows 9 model combinations (3 LLMs Ã— 3 vision encoders).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RAW DATA: Interpretability percentages per layer\n",
        "# Format: {\"llm+encoder\": {layer: percentage}}\n",
        "# =============================================================================\n",
        "\n",
        "# Nearest Neighbors (Static V-Lens) - LLM Judge evaluated\n",
        "NN_DATA = {\n",
        "    \"llama3-8b+dinov2-large-336\": {0: 20.33, 1: 17.0, 2: 19.0, 3: 18.0, 4: 20.0, 8: 15.0, 12: 19.0, 16: 20.0, 20: 24.0, 24: 19.0, 28: 22.0, 32: 12.0},\n",
        "    \"llama3-8b+siglip\": {0: 23.33, 1: 30.0, 2: 24.0, 3: 29.0, 4: 28.0, 8: 31.0, 12: 31.0, 16: 29.0, 20: 34.0, 24: 27.0, 28: 27.0, 32: 29.0},\n",
        "    \"llama3-8b+vit-l-14-336\": {0: 35.33, 1: 30.0, 2: 37.0, 3: 32.0, 4: 29.0, 8: 37.0, 12: 44.0, 16: 51.0, 20: 52.0, 24: 47.0, 28: 43.0, 32: 21.0},\n",
        "    \"olmo-7b+dinov2-large-336\": {0: 42.0, 1: 45.0, 2: 40.0, 3: 41.0, 4: 47.0, 8: 56.0, 12: 61.0, 16: 67.0, 20: 70.0, 24: 67.0, 28: 67.0, 32: 33.0},\n",
        "    \"olmo-7b+siglip\": {0: 41.67, 1: 39.0, 2: 38.0, 3: 28.0, 4: 39.0, 8: 45.0, 12: 55.0, 16: 49.0, 20: 55.0, 24: 56.0, 28: 53.0, 32: 22.0},\n",
        "    \"olmo-7b+vit-l-14-336\": {0: 55.0, 1: 52.0, 2: 56.0, 3: 60.0, 4: 59.0, 8: 60.0, 12: 62.0, 16: 62.0, 20: 63.0, 24: 59.0, 28: 62.0, 32: 35.0},\n",
        "    \"qwen2-7b+dinov2-large-336\": {0: 7.0, 1: 10.0, 2: 9.0, 3: 7.0, 4: 9.0, 8: 11.0, 12: 11.0, 16: 11.0, 20: 12.0, 24: 14.0, 28: 9.0},\n",
        "    \"qwen2-7b+siglip\": {0: 5.33, 1: 4.0, 2: 5.0, 3: 5.0, 4: 4.0, 8: 3.0, 12: 5.0, 16: 4.0, 20: 5.0, 24: 5.0, 28: 9.0},\n",
        "    \"qwen2-7b+vit-l-14-336\": {0: 17.67, 1: 15.0, 2: 9.0, 3: 13.0, 4: 15.0, 8: 18.0, 12: 18.0, 16: 9.0, 20: 8.0, 24: 16.0, 28: 10.0},\n",
        "}\n",
        "\n",
        "# Logit Lens - LLM Judge evaluated\n",
        "LOGITLENS_DATA = {\n",
        "    \"llama3-8b+dinov2-large-336\": {0: 9.0, 1: 7.0, 2: 9.0, 3: 11.0, 4: 5.0, 8: 10.0, 12: 10.0, 16: 11.0, 20: 9.0, 24: 7.0, 28: 7.0, 29: 9.0, 30: 13.0, 31: 7.0, 32: 7.0},\n",
        "    \"llama3-8b+siglip\": {0: 9.0, 1: 9.0, 2: 10.0, 3: 8.0, 4: 12.0, 8: 10.0, 12: 9.0, 16: 9.0, 20: 13.0, 24: 14.0, 28: 8.0, 29: 10.0, 30: 9.0, 31: 9.0, 32: 7.0},\n",
        "    \"llama3-8b+vit-l-14-336\": {0: 13.0, 1: 10.0, 2: 10.0, 3: 11.0, 4: 14.0, 8: 10.0, 12: 7.0, 16: 12.0, 20: 26.0, 24: 50.0, 28: 52.0, 29: 62.0, 30: 64.0, 31: 76.0, 32: 81.0},\n",
        "    \"olmo-7b+dinov2-large-336\": {0: 11.0, 1: 13.0, 2: 13.0, 3: 13.0, 4: 17.0, 8: 15.0, 12: 23.0, 16: 39.0, 20: 61.0, 24: 78.0, 28: 76.0, 29: 78.0, 30: 69.0, 31: 56.0, 32: 32.0},\n",
        "    \"olmo-7b+siglip\": {0: 14.0, 1: 20.0, 2: 15.0, 3: 21.0, 4: 16.0, 8: 20.0, 12: 22.0, 16: 26.0, 20: 52.0, 24: 69.0, 28: 83.0, 29: 86.0, 30: 82.0, 31: 63.0, 32: 43.0},\n",
        "    \"olmo-7b+vit-l-14-336\": {0: 11.0, 1: 8.0, 2: 18.0, 3: 19.0, 4: 19.0, 8: 22.0, 12: 25.0, 16: 23.0, 20: 49.0, 24: 75.0, 28: 78.0, 29: 82.0, 30: 74.0, 31: 59.0, 32: 31.0},\n",
        "    \"qwen2-7b+dinov2-large-336\": {0: 15.0, 1: 9.0, 2: 10.0, 3: 12.0, 4: 9.0, 8: 7.0, 12: 8.0, 16: 13.0, 20: 14.0, 24: 25.0, 25: 34.0, 26: 42.0, 27: 56.0, 28: 45.0},\n",
        "    \"qwen2-7b+siglip\": {0: 8.0, 1: 7.0, 2: 9.0, 3: 7.0, 4: 9.0, 8: 8.0, 12: 6.0, 16: 8.0, 20: 7.0, 24: 11.0, 25: 6.0, 26: 11.0, 27: 6.0, 28: 12.0},\n",
        "    \"qwen2-7b+vit-l-14-336\": {0: 6.0, 1: 4.0, 2: 6.0, 3: 2.0, 4: 3.0, 8: 7.0, 12: 12.0, 16: 8.0, 20: 9.0, 24: 43.0, 25: 51.0, 26: 59.0, 27: 78.0, 28: 71.0},\n",
        "}\n",
        "\n",
        "# Contextual NN (Contextual V-Lens) - LLM Judge evaluated\n",
        "# Layer 0 = static NN (same as NN_DATA at layer 0), Layer 1+ = contextual embeddings\n",
        "CONTEXTUAL_DATA = {\n",
        "    \"llama3-8b+dinov2-large-336\": {0: 20.33, 1: 82.0, 2: 81.0, 4: 82.0, 8: 82.0, 16: 82.0, 24: 83.33},\n",
        "    \"llama3-8b+siglip\": {0: 23.33, 1: 63.0, 2: 60.0, 4: 65.0, 8: 62.0, 16: 58.0, 24: 63.89},\n",
        "    \"llama3-8b+vit-l-14-336\": {0: 35.33, 1: 82.0, 2: 84.0, 4: 79.0, 8: 82.0, 16: 82.0, 24: 70.0},\n",
        "    \"olmo-7b+dinov2-large-336\": {0: 42.0, 1: 81.0, 2: 79.0, 4: 80.0, 8: 80.0, 16: 81.0, 24: 82.43},\n",
        "    \"olmo-7b+siglip\": {0: 41.67, 1: 63.0, 2: 64.0, 4: 65.0, 8: 64.0, 16: 65.0, 24: 60.87},\n",
        "    \"olmo-7b+vit-l-14-336\": {0: 54.67, 1: 71.0, 2: 71.0, 4: 70.0, 8: 71.0, 16: 70.0, 24: 69.7},\n",
        "    \"qwen2-7b+dinov2-large-336\": {0: 7.0, 1: 79.0, 2: 81.0, 4: 83.0, 8: 81.0, 16: 80.0, 24: 82.81},\n",
        "    \"qwen2-7b+siglip\": {0: 5.33, 1: 65.0, 2: 64.0, 4: 62.0, 8: 68.0, 16: 69.0, 24: 66.67},\n",
        "    \"qwen2-7b+vit-l-14-336\": {0: 17.67, 1: 76.0, 2: 80.0, 4: 78.0, 8: 76.0, 16: 79.0, 24: 78.0, 26: 71.43},\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PLOTTING CONFIG: Colors, markers, labels\n",
        "# =============================================================================\n",
        "\n",
        "# Display names for paper\n",
        "LLM_DISPLAY_NAMES = {\n",
        "    'llama3-8b': 'Llama3-8B',\n",
        "    'olmo-7b': 'OLMo-7B',\n",
        "    'qwen2-7b': 'Qwen2-7B'\n",
        "}\n",
        "\n",
        "ENCODER_DISPLAY_NAMES = {\n",
        "    'vit-l-14-336': 'CLIP ViT-L/14',\n",
        "    'siglip': 'SigLIP',\n",
        "    'dinov2-large-336': 'DINOv2'\n",
        "}\n",
        "\n",
        "# Order for consistent legend\n",
        "LLM_ORDER = ['olmo-7b', 'llama3-8b', 'qwen2-7b']\n",
        "ENCODER_ORDER = ['vit-l-14-336', 'siglip', 'dinov2-large-336']\n",
        "\n",
        "# Color scheme: each LLM gets a color family, encoders get shades\n",
        "LLM_BASE_COLORS = {\n",
        "    'olmo-7b': plt.cm.Blues,\n",
        "    'llama3-8b': plt.cm.Greens,\n",
        "    'qwen2-7b': plt.cm.Reds\n",
        "}\n",
        "ENCODER_SHADE_INDICES = [0.5, 0.7, 0.9]\n",
        "\n",
        "# Markers for each encoder\n",
        "ENCODER_MARKERS = {\n",
        "    'vit-l-14-336': '*',       # star (filled)\n",
        "    'siglip': 'o',             # circle (hollow)\n",
        "    'dinov2-large-336': '^'    # triangle (filled)\n",
        "}\n",
        "ENCODER_MARKER_FACECOLORS = {\n",
        "    'vit-l-14-336': None,      # filled\n",
        "    'siglip': 'none',          # hollow\n",
        "    'dinov2-large-336': None   # filled\n",
        "}\n",
        "\n",
        "def get_color_map():\n",
        "    \"\"\"Generate color mapping for all model combinations.\"\"\"\n",
        "    color_map = {}\n",
        "    for llm in LLM_ORDER:\n",
        "        base_cmap = LLM_BASE_COLORS[llm]\n",
        "        for enc_idx, encoder in enumerate(ENCODER_ORDER):\n",
        "            color_map[(llm, encoder)] = base_cmap(ENCODER_SHADE_INDICES[enc_idx])\n",
        "    return color_map\n",
        "\n",
        "def parse_model_key(key):\n",
        "    \"\"\"Parse 'llm+encoder' string into (llm, encoder) tuple.\"\"\"\n",
        "    parts = key.split('+')\n",
        "    return parts[0], parts[1]\n",
        "\n",
        "def get_display_label(llm, encoder):\n",
        "    \"\"\"Get display label for legend.\"\"\"\n",
        "    llm_label = LLM_DISPLAY_NAMES.get(llm, llm)\n",
        "    encoder_label = ENCODER_DISPLAY_NAMES.get(encoder, encoder)\n",
        "    return f\"{llm_label} + {encoder_label}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PLOTTING FUNCTION: Unified 3-panel figure\n",
        "# =============================================================================\n",
        "\n",
        "def create_unified_lineplot(nn_data, logitlens_data, contextual_data, \n",
        "                            output_path=None, figsize=(18, 5)):\n",
        "    \"\"\"\n",
        "    Create unified figure with 3 subplots showing interpretability across layers.\n",
        "    \n",
        "    Args:\n",
        "        nn_data: Dict of nearest neighbors data\n",
        "        logitlens_data: Dict of logit lens data  \n",
        "        contextual_data: Dict of contextual NN data\n",
        "        output_path: Path to save figure (optional)\n",
        "        figsize: Figure size tuple\n",
        "    \"\"\"\n",
        "    color_map = get_color_map()\n",
        "    \n",
        "    # Create figure with 3 subplots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
        "    \n",
        "    subplot_configs = [\n",
        "        {'ax': axes[0], 'data': nn_data, 'title': '(a) Static V-Lens (NN)', 'xlabel': 'Layer'},\n",
        "        {'ax': axes[1], 'data': logitlens_data, 'title': '(b) Logit Lens', 'xlabel': 'Layer'},\n",
        "        {'ax': axes[2], 'data': contextual_data, 'title': '(c) Contextual V-Lens', 'xlabel': 'Layer'},\n",
        "    ]\n",
        "    \n",
        "    handles_dict = {}\n",
        "    \n",
        "    for config in subplot_configs:\n",
        "        ax = config['ax']\n",
        "        data = config['data']\n",
        "        \n",
        "        if not data:\n",
        "            continue\n",
        "        \n",
        "        # Get all layers\n",
        "        all_layers = set()\n",
        "        for key, layer_data in data.items():\n",
        "            all_layers.update(layer_data.keys())\n",
        "        all_layers = sorted(list(all_layers))\n",
        "        \n",
        "        # Plot lines for each model combination\n",
        "        for llm in LLM_ORDER:\n",
        "            for encoder in ENCODER_ORDER:\n",
        "                key = f\"{llm}+{encoder}\"\n",
        "                if key not in data:\n",
        "                    continue\n",
        "                \n",
        "                layer_data = data[key]\n",
        "                layers = sorted(layer_data.keys())\n",
        "                values = [layer_data[l] for l in layers]\n",
        "                \n",
        "                if len(layers) == 0:\n",
        "                    continue\n",
        "                \n",
        "                label = get_display_label(llm, encoder)\n",
        "                marker = ENCODER_MARKERS.get(encoder, 'o')\n",
        "                marker_facecolor = ENCODER_MARKER_FACECOLORS.get(encoder)\n",
        "                color = color_map[(llm, encoder)]\n",
        "                \n",
        "                if marker_facecolor is not None:\n",
        "                    line, = ax.plot(layers, values, marker=marker, color=color,\n",
        "                                   markerfacecolor=marker_facecolor,\n",
        "                                   markeredgewidth=2, linewidth=2.5, markersize=10)\n",
        "                else:\n",
        "                    line, = ax.plot(layers, values, marker=marker, color=color,\n",
        "                                   linewidth=2.5, markersize=10)\n",
        "                \n",
        "                if label not in handles_dict:\n",
        "                    handles_dict[label] = line\n",
        "        \n",
        "        # Customize subplot\n",
        "        ax.set_xlabel(config['xlabel'], fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel('Interpretability %', fontsize=14, fontweight='bold')\n",
        "        ax.set_title(config['title'], fontsize=16, fontweight='bold', pad=10)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_ylim(0, 100)\n",
        "        ax.tick_params(axis='both', labelsize=11)\n",
        "        \n",
        "        if all_layers:\n",
        "            ax.set_xlim(min(all_layers) - 0.5, max(all_layers) + 0.5)\n",
        "            # Show subset of x-ticks if too many\n",
        "            if len(all_layers) > 10:\n",
        "                step = max(1, len(all_layers) // 8)\n",
        "                shown_layers = all_layers[::step]\n",
        "                if all_layers[-1] not in shown_layers:\n",
        "                    shown_layers.append(all_layers[-1])\n",
        "                ax.set_xticks(shown_layers)\n",
        "            else:\n",
        "                ax.set_xticks(all_layers)\n",
        "    \n",
        "    # Create shared legend at bottom\n",
        "    ordered_handles = []\n",
        "    ordered_labels = []\n",
        "    for llm in LLM_ORDER:\n",
        "        for encoder in ENCODER_ORDER:\n",
        "            label = get_display_label(llm, encoder)\n",
        "            if label in handles_dict:\n",
        "                ordered_handles.append(handles_dict[label])\n",
        "                ordered_labels.append(label)\n",
        "    \n",
        "    fig.legend(ordered_handles, ordered_labels,\n",
        "              loc='lower center',\n",
        "              bbox_to_anchor=(0.5, -0.12),\n",
        "              ncol=3,\n",
        "              fontsize=12,\n",
        "              framealpha=0.9,\n",
        "              columnspacing=2.0,\n",
        "              handlelength=2.5)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(bottom=0.22, wspace=0.25)\n",
        "    \n",
        "    if output_path:\n",
        "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Saved: {output_path}\")\n",
        "        # Also save PNG\n",
        "        png_path = Path(output_path).with_suffix('.png')\n",
        "        plt.savefig(png_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Saved: {png_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Figure 1: Unified interpretability plot\n",
        "fig1 = create_unified_lineplot(\n",
        "    NN_DATA, \n",
        "    LOGITLENS_DATA, \n",
        "    CONTEXTUAL_DATA,\n",
        "    output_path=OUTPUT_DIR / 'fig1_unified_interpretability.pdf'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Figure 2: Individual Method Plots\n",
        "\n",
        "Separate plots for each method with more detail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_single_lineplot(data, title, xlabel='Layer', ylabel='Interpretability %',\n",
        "                           output_path=None, figsize=(10, 6)):\n",
        "    \"\"\"\n",
        "    Create a single line plot for one method.\n",
        "    \"\"\"\n",
        "    color_map = get_color_map()\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    \n",
        "    # Get all layers\n",
        "    all_layers = set()\n",
        "    for key, layer_data in data.items():\n",
        "        all_layers.update(layer_data.keys())\n",
        "    all_layers = sorted(list(all_layers))\n",
        "    \n",
        "    # Plot lines\n",
        "    for llm in LLM_ORDER:\n",
        "        for encoder in ENCODER_ORDER:\n",
        "            key = f\"{llm}+{encoder}\"\n",
        "            if key not in data:\n",
        "                continue\n",
        "            \n",
        "            layer_data = data[key]\n",
        "            layers = sorted(layer_data.keys())\n",
        "            values = [layer_data[l] for l in layers]\n",
        "            \n",
        "            if len(layers) == 0:\n",
        "                continue\n",
        "            \n",
        "            label = get_display_label(llm, encoder)\n",
        "            marker = ENCODER_MARKERS.get(encoder, 'o')\n",
        "            marker_facecolor = ENCODER_MARKER_FACECOLORS.get(encoder)\n",
        "            color = color_map[(llm, encoder)]\n",
        "            \n",
        "            if marker_facecolor is not None:\n",
        "                ax.plot(layers, values, marker=marker, label=label, color=color,\n",
        "                       markerfacecolor=marker_facecolor,\n",
        "                       markeredgewidth=1.5, linewidth=2, markersize=8)\n",
        "            else:\n",
        "                ax.plot(layers, values, marker=marker, label=label, color=color,\n",
        "                       linewidth=2, markersize=8)\n",
        "    \n",
        "    ax.set_xlabel(xlabel, fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel(ylabel, fontsize=12, fontweight='bold')\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold', pad=15)\n",
        "    ax.legend(loc='best', fontsize=9, framealpha=0.9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_ylim(0, 100)\n",
        "    \n",
        "    if all_layers:\n",
        "        ax.set_xlim(min(all_layers) - 0.5, max(all_layers) + 0.5)\n",
        "        ax.set_xticks(all_layers)\n",
        "        if len(all_layers) > 15:\n",
        "            ax.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if output_path:\n",
        "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Saved: {output_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Individual plots\n",
        "fig_nn = create_single_lineplot(\n",
        "    NN_DATA,\n",
        "    title='Static V-Lens (Nearest Neighbors) Interpretability',\n",
        "    output_path=OUTPUT_DIR / 'fig_nn_interpretability.pdf'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig_logitlens = create_single_lineplot(\n",
        "    LOGITLENS_DATA,\n",
        "    title='Logit Lens Interpretability',\n",
        "    output_path=OUTPUT_DIR / 'fig_logitlens_interpretability.pdf'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig_contextual = create_single_lineplot(\n",
        "    CONTEXTUAL_DATA,\n",
        "    title='Contextual V-Lens Interpretability',\n",
        "    output_path=OUTPUT_DIR / 'fig_contextual_interpretability.pdf'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Data Tables\n",
        "\n",
        "Print the data in tabular format for reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_data_table(data, title):\n",
        "    \"\"\"Print data in a formatted table.\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{title}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Get all layers\n",
        "    all_layers = set()\n",
        "    for key, layer_data in data.items():\n",
        "        all_layers.update(layer_data.keys())\n",
        "    all_layers = sorted(list(all_layers))\n",
        "    \n",
        "    # Print header\n",
        "    print(f\"{'Model':<35}\", end=\"\")\n",
        "    for layer in all_layers:\n",
        "        print(f\"{'L'+str(layer):>7}\", end=\"\")\n",
        "    print()\n",
        "    print(\"-\" * (35 + len(all_layers) * 7))\n",
        "    \n",
        "    # Print data\n",
        "    for llm in LLM_ORDER:\n",
        "        for encoder in ENCODER_ORDER:\n",
        "            key = f\"{llm}+{encoder}\"\n",
        "            if key not in data:\n",
        "                continue\n",
        "            label = get_display_label(llm, encoder)\n",
        "            print(f\"{label:<35}\", end=\"\")\n",
        "            for layer in all_layers:\n",
        "                value = data[key].get(layer)\n",
        "                if value is not None:\n",
        "                    print(f\"{value:>6.1f}%\", end=\"\")\n",
        "                else:\n",
        "                    print(f\"{'---':>7}\", end=\"\")\n",
        "            print()\n",
        "\n",
        "print_data_table(NN_DATA, \"Static V-Lens (Nearest Neighbors) Data\")\n",
        "print_data_table(LOGITLENS_DATA, \"Logit Lens Data\")\n",
        "print_data_table(CONTEXTUAL_DATA, \"Contextual V-Lens Data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_summary_stats(data, name):\n",
        "    \"\"\"Compute summary statistics for a dataset.\"\"\"\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for llm in LLM_ORDER:\n",
        "        for encoder in ENCODER_ORDER:\n",
        "            key = f\"{llm}+{encoder}\"\n",
        "            if key not in data:\n",
        "                continue\n",
        "            values = list(data[key].values())\n",
        "            label = get_display_label(llm, encoder)\n",
        "            print(f\"  {label}:\")\n",
        "            print(f\"    Layer 0: {data[key].get(0, 'N/A'):.1f}%\" if 0 in data[key] else \"    Layer 0: N/A\")\n",
        "            print(f\"    Max: {max(values):.1f}% | Min: {min(values):.1f}% | Mean: {np.mean(values):.1f}%\")\n",
        "\n",
        "compute_summary_stats(NN_DATA, \"Static V-Lens\")\n",
        "compute_summary_stats(LOGITLENS_DATA, \"Logit Lens\")\n",
        "compute_summary_stats(CONTEXTUAL_DATA, \"Contextual V-Lens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Key Findings\n",
        "\n",
        "Based on the data above:\n",
        "\n",
        "1. **Static V-Lens (Layer 0)**: OLMo + CLIP shows highest interpretability (~55%), Qwen2 shows lowest (~5-17%)\n",
        "\n",
        "2. **Logit Lens**: Shows interpretability increasing dramatically in final layers for most models (e.g., Llama3+CLIP goes from 13% to 81%)\n",
        "\n",
        "3. **Contextual V-Lens**: Even models with low static interpretability (Qwen2) achieve high interpretability (>75%) when using contextual embeddings from layer 1+\n",
        "\n",
        "4. **Encoder Effect**: CLIP ViT-L/14 generally provides higher interpretability than SigLIP and DINOv2 at layer 0, but DINOv2 catches up in later contextual layers\n",
        "\n",
        "5. **LLM Effect**: OLMo shows consistently high interpretability across all methods, while Qwen2 shows the largest gap between static and contextual approaches\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
